<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/gh/alpinejs/alpine@v2.x.x/dist/alpine.min.js" defer></script>
    <script src="https://unpkg.com/htmx.org"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.7.570/pdf.min.js"></script>
    <script src="/static/js/validateupload.js"></script>
    <title>AI Document Summarization Tool</title>

 
<body class="bg-black text-white flex flex-col min-h-screen">
    <header >
        <div class="antialiased bg-green-900 ">
        <div class="w-full bg-green-900">
          <div x-data="{ open: true }" class="flex flex-col max-w-screen-xl px-4 mx-auto md:items-center md:justify-between md:flex-row md:px-6 lg:px-8">
            <div class="flex flex-row items-center justify-between p-4">
              <a href="/" class="text-lg font-semibold ">SUMMARIZE AI</a>
              <button class="rounded-lg md:hidden focus:outline-none focus:shadow-outline" @click="open = !open">
                <svg fill="currentColor" viewBox="0 0 20 20" class="w-6 h-6">
                  <path x-show="!open" fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM9 15a1 1 0 011-1h6a1 1 0 110 2h-6a1 1 0 01-1-1z" clip-rule="evenodd"></path>
                  <path x-show="open" fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path>
                </svg>
              </button>
            </div>
            <nav :class="{'flex': open, 'hidden': !open}" class="flex-col flex-grow hidden pb-4 md:pb-0 md:flex md:justify-end md:flex-row">
              <a class="px-4 py-2 mt-2 text-sm font-semibold bg-transparent rounded-lg dark-mode:bg-transparent dark-mode:hover:bg-gray-600 dark-mode:focus:bg-gray-600 dark-mode:focus:text-white dark-mode:hover:text-white dark-mode:text-gray-200 md:mt-0 md:ml-4 hover:text-gray-900 focus:text-gray-900 hover:bg-gray-200 focus:bg-gray-200 focus:outline-none focus:shadow-outline" href="/uploadtext">Enter Text</a>
              <a class="px-4 py-2 mt-2 text-sm font-semibold bg-transparent rounded-lg dark-mode:bg-transparent dark-mode:hover:bg-gray-600 dark-mode:focus:bg-gray-600 dark-mode:focus:text-white dark-mode:hover:text-white dark-mode:text-gray-200 md:mt-0 md:ml-4 hover:text-gray-900 focus:text-gray-900 hover:bg-gray-200 focus:bg-gray-200 focus:outline-none focus:shadow-outline" href="/uploadfile">Upload File</a>
              <a class="px-4 py-2 mt-2 text-sm font-semibold bg-transparent rounded-lg dark-mode:bg-transparent dark-mode:hover:bg-gray-600 dark-mode:focus:bg-gray-600 dark-mode:focus:text-white dark-mode:hover:text-white dark-mode:text-gray-200 md:mt-0 md:ml-4 hover:text-gray-900 focus:text-gray-900 hover:bg-gray-200 focus:bg-gray-200 focus:outline-none focus:shadow-outline" href="/about">About</a>
              <a class="px-4 py-2 mt-2 text-sm font-semibold bg-transparent rounded-lg dark-mode:bg-transparent dark-mode:hover:bg-gray-600 dark-mode:focus:bg-gray-600 dark-mode:focus:text-white dark-mode:hover:text-white dark-mode:text-gray-200 md:mt-0 md:ml-4 hover:text-gray-900 focus:text-gray-900 hover:bg-gray-200 focus:bg-gray-200 focus:outline-none focus:shadow-outline" href="/contact">Contact</a>
                
                </div>
              </div>    
            </nav>
          </div>
      
      </div>
        </div>
      </header>
 <section class="flex-grow p-8 w-full flex flex-col items-center justify-center">
        <div class="max-w-4xl mx-auto px-4 py-12">
            <h2 class="text-3xl font-semibold mb-4 text-center">About</h2>
            <p class=" mt-4 text-lg">
                For my capstone for NSCC It Programming, I initially wanted to create a tool that uses AI to search through Nova Scotian case law. The idea was it would expedite the process of finding relevant case law to your situation. The extreme difficulty in gathering the training data of cases, lead me to a different project idea for the capstone.  
            </p>
            <p class="mt-4 text-lg">
                I decided to create a tool to summarize documents. Many pre built solutions exist for this, but I wanted to take a mode, train it on a dataset, and deploy the trained model.  I followed this video by Julian Simon, a Hugging Face employee <a style="color: aqua;" href="https://www.youtube.com/watch?v=tc87-ZKWm78&list=LL&index=4"> Summarizing legal documents with Hugging Face and Amazon SageMaker </a>. It helped explain and run through what was happening to allow the training. 
            </p>
            <p class="mt-4 text-lg">
                After the model was trained, I worked through getting it set up locally, and experimenting with how the model worked.  I discovered that the amount of tokens the model would accept is quite limited, so the text input had to be broken into small chunks, summarized, joined together and then summarized again. It also is important to note that using the Hugging Face Transformers library with the "summarization" setting produced very inferior results. 
          </p>
          <p class="mt-4 text-lg">
                The next step was getting the model deployed non locally. This was a very long and difficult process for me. Simply using AWS Sagemaker model inference was not an option because of the high cost. Creating a custom docker image with the model helped, but getting gpu accelerated use of the container was very tough. So the solutions I found were either too expensive, too slow, or too poor. I tried using Hugging Face's model inference endpoints and that was an improvement in compute time, but still was using the basic Transformers "summarization" pipeline that had poor and inconsistent results. I tried setting up the model with a default handler.py, but was not having that picked up until i rebuilt the model endpoint with the container type set to "Default", not "Text Generation". Success! The model worked as well, and much faster then it did locally. 
          </p>
            <p class="mt-4 text-lg">
                Then I built a web site to connect to the endpoint, which I did using go, tailwind, and htmx. This was relatively straightforward, the concurrency model for go makes the chunking of the request very fun!  Then to deploy the website I knew I wanted to dockerize it, use github actions to automate the build, and deploy it to elastic beanstalk on AWS. This video <a style="color: aqua;" href="https://www.youtube.com/watch?v=Hv5UcBYseus"> Build a Docker Image and Publish It to AWS ECR using Github Actions </a> was very helpful in that, although some things had to be modified for my use case. My go project needed to serve templates, and I wanted the build pipeline to also deploy the new image from ECR to the Elastic beanstalk instance.
            </p>
            <div class="mt-4 max-w-4xl mx-auto">
              <h2 class="text-3xl font-semibold mb-4 text-center">Concurrency of Requests</h2>
              <div class="bg-gray-800 p-6 rounded-lg shadow-lg">
                  <pre class="language-go"><code class="language-go text-blue-300">
      type indexSummary struct {
        index   int
        summary string
      }
      
      chunks := ChunkText(input, chunkSize)
      
      <span class="text-green-400">// This used to be a channel of strings, but they would return unsorted,</span>
      <span class="text-green-400">// so they are a channel of structs that contain an index to sort</span>
      summaryChannel := make(chan indexSummary, len(chunks))
      var waitGroup sync.WaitGroup
      
      for i, chunk := range chunks {
        waitGroup.Add(1)
      
        go func(index int, chunk string) {
          defer waitGroup.Done()
          summaryChannel &lt;- indexSummary{
              index: index, summary: SendModelRequest("summarize: " + chunk)}
        }(i, chunk)
      }
      
      <span class="text-green-400">// After all the goroutines are executed, the summary Channel</span>
      <span class="text-green-400">// Containing a slice of structs is ready to be sorted.</span>
      waitGroup.Wait()
      close(summaryChannel)</code></pre>
              </div>
          </div>
          <p class="mt-4 text-lg">
            I was finding the summaries that the model would create would be short. That is mostly because during the fine tuning of the google-flan-T5 model i used the title of each document as the output. I then fine tuned a facebook-bart-cnn model which was already built for summarization on the <a style="color: aqua;" href="https://huggingface.co/datasets/billsum/viewer/default/test?row=0">Billsum</a> dataset. On this fine tuning job i used the 'text' as the input, and the 'summary' as a output. This caused this new model to produce a much better summary then the first attempt.
        </p>
        </div>
 </section>
</body>
</html>